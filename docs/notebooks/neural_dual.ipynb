{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Kantorovich Dual using Input Convex Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we explore how to learn the solution of the Kantorovich dual based on parameterizing the two dual potentials $f$ and $g$ with two [input convex neural networks (ICNN)](http://proceedings.mlr.press/v70/amos17b/amos17b.pdf), a method developed by [Makkuva et al. (2020)](http://proceedings.mlr.press/v119/makkuva20a/makkuva20a.pdf). For more insights on the approach itself, we refer the user to the original publication.\n",
    "Given dataloaders containing samples of the *source* and the *target* distribution, `OTT`'s `NeuralDualSolver` find the pair of optimal potentials $f$ and $g$ to solve the corresponding dual of the optimal transport problem. Once a solution has been found, this can be used to transport unseen source data samples to its target distribution (or vice-versa) or compute the corresponding distance between new source and target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ott-jax in /Users/bunnech/Documents/PhD/Projects/ott (0.2.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from ott-jax) (1.0.0)\n",
      "Requirement already satisfied: jax>=0.1.67 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from ott-jax) (0.3.1)\n",
      "Requirement already satisfied: jaxlib>=0.1.47 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from ott-jax) (0.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.4 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from ott-jax) (1.22.3)\n",
      "Requirement already satisfied: matplotlib>=2.0.1 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from ott-jax) (3.5.1)\n",
      "Requirement already satisfied: flax>=0.4.0 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from ott-jax) (0.4.0)\n",
      "Requirement already satisfied: optax>=0.1.1 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from ott-jax) (0.1.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from ott-jax) (1.11.0)\n",
      "Collecting tqdm>=4.63.0\n",
      "  Using cached tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: six in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from absl-py>=0.7.0->ott-jax) (1.16.0)\n",
      "Requirement already satisfied: msgpack in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from flax>=0.4.0->ott-jax) (1.0.3)\n",
      "Requirement already satisfied: opt-einsum in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from jax>=0.1.67->ott-jax) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from jax>=0.1.67->ott-jax) (4.1.1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from jax>=0.1.67->ott-jax) (1.8.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from jaxlib>=0.1.47->ott-jax) (2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.0.1->ott-jax) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.0.1->ott-jax) (4.30.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.0.1->ott-jax) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.0.1->ott-jax) (3.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.0.1->ott-jax) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.0.1->ott-jax) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.0.1->ott-jax) (21.3)\n",
      "Requirement already satisfied: chex>=0.0.4 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from optax>=0.1.1->ott-jax) (0.1.1)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from chex>=0.0.4->optax>=0.1.1->ott-jax) (0.1.6)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/bunnech/miniforge3/lib/python3.9/site-packages (from chex>=0.0.4->optax>=0.1.1->ott-jax) (0.11.2)\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "Successfully installed tqdm-4.63.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ott-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bunnech/miniforge3/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from ott.tools.sinkhorn_divergence import sinkhorn_divergence\n",
    "from ott.geometry import pointcloud\n",
    "from ott.core.neuraldual import NeuralDualSolver\n",
    "from ott.core import icnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define some helper functions which we use for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ot_map(neural_dual, source, target, potential='g'):\n",
    "    \"\"\"Plot data and learned optimal transport map.\"\"\"\n",
    "\n",
    "    def draw_arrows(a, b):\n",
    "        plt.arrow(a[0], a[1], b[0] - a[0], b[1] - a[1],\n",
    "                  color=[0.5, 0.5, 1], alpha=0.3)\n",
    "\n",
    "    grad_state_s = neural_dual.transport(source, potential)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.scatter(target[:, 0], target[:, 1], color='#A7BED3',\n",
    "               alpha=0.5, label=r'$target$')\n",
    "    ax.scatter(source[:, 0], source[:, 1], color='#1A254B',\n",
    "               alpha=0.5, label=r'$source$')\n",
    "    if potential == 'g':\n",
    "        ax.scatter(grad_state_s[:, 0], grad_state_s[:, 1], color='#F2545B',\n",
    "               alpha=0.5, label=r'$\\nabla g(source)$')\n",
    "    elif potential == 'f':\n",
    "        ax.scatter(grad_state_s[:, 0], grad_state_s[:, 1], color='#F2545B',\n",
    "                   alpha=0.5, label=r'$\\nabla f(target)$')\n",
    "    else:\n",
    "        raise ValueError('Potential is not specified correctly.')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    for i in range(source.shape[0]):\n",
    "        draw_arrows(source[i, :], grad_state_s[i, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sinkhorn_loss(x, y, epsilon=0.1, power=2.0):\n",
    "    \"\"\"Computes transport between (x, a) and (y, b) via Sinkhorn algorithm.\"\"\"\n",
    "    a = jnp.ones(len(x)) / len(x)\n",
    "    b = jnp.ones(len(y)) / len(y)\n",
    "\n",
    "    sdiv = sinkhorn_divergence(pointcloud.PointCloud, x, y, power=power,\n",
    "                               epsilon=epsilon, a=a, b=b)\n",
    "    return sdiv.divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the `NeuralDual` to compute the transport between toy datasets. In this tutorial, the user can choose between the datasets `simple` (data clustered in one center), `circle` (two-dimensional Gaussians arranged on a circle), `square_five` (two-dimensional Gaussians on a square with one Gaussian in the center), and `square_four` (two-dimensional Gaussians in the corners of a rectangle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(IterableDataset):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.create_sample_generators()\n",
    "\n",
    "    def create_sample_generators(self, scale=5.0, variance=0.5):\n",
    "        # given name of dataset, select centers\n",
    "        if self.name == \"simple\":\n",
    "            centers = np.array([0, 0])\n",
    "\n",
    "        elif self.name == \"circle\":\n",
    "            centers = np.array(\n",
    "                [\n",
    "                    (1, 0),\n",
    "                    (-1, 0),\n",
    "                    (0, 1),\n",
    "                    (0, -1),\n",
    "                    (1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n",
    "                    (1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n",
    "                    (-1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n",
    "                    (-1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        elif self.name == \"square_five\":\n",
    "            centers = np.array([[0, 0], [1, 1], [-1, 1], [-1, -1], [1, -1]])\n",
    "\n",
    "        elif self.name == \"square_four\":\n",
    "            centers = np.array([[1, 0], [0, 1], [-1, 0], [0, -1]])\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # create generator which randomly picks center and adds noise\n",
    "        centers = scale * centers\n",
    "        while True:\n",
    "            center = centers[np.random.choice(len(centers))]\n",
    "            point = center + variance**2 * np.random.randn(2)\n",
    "\n",
    "            yield point\n",
    "\n",
    "\n",
    "def load_toy_data(name_source: str,\n",
    "                  name_target: str,\n",
    "                  batch_size: int = 1024,\n",
    "                  valid_batch_size: int = 1024):\n",
    "    dataloaders = (\n",
    "      iter(DataLoader(ToyDataset(name_source), batch_size=batch_size)),\n",
    "      iter(DataLoader(ToyDataset(name_target), batch_size=batch_size)),\n",
    "      iter(DataLoader(ToyDataset(name_source), batch_size=valid_batch_size)),\n",
    "      iter(DataLoader(ToyDataset(name_target), batch_size=valid_batch_size)),\n",
    "    )\n",
    "    input_dim = 2\n",
    "    return dataloaders, input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Neural Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve the neural dual, we need to define our dataloaders. The only requirement is that the corresponding source and target train and validation datasets are *iterators*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataloader_source, dataloader_target, _, _), input_dim = load_toy_data('simple', 'circle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the architectures parameterizing the dual potentials $f$ and $g$. These need to be parameterized by ICNNs. You can adapt the size of the ICNNs by passing a sequence containing hidden layer sizes. While ICNNs are by default containing partially positive weights, we can solve the `NeuralDual` using approximations to this positivity constraint (via weight clipping and a weight penalization). For this, set `positive weights` to True in both the `ICNN` architecture and `NeuralDualSolver` configuration. For more details on how to customize the ICNN architectures, we refer you to the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "icnn_f = icnn.ICNN(dim_hidden=[64, 64, 64, 64])\n",
    "icnn_g = icnn.ICNN(dim_hidden=[64, 64, 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize the `NeuralDualSolver` by passing the two ICNN models parameterizing $f$ and $g$, as well as by specifying the input dimensions of the data and the number of training iterations to execute. Once the `NeuralDualSolver` is initialized, we can obtain the `NeuralDual` by passing the corresponding dataloaders to it, which will subsequently return the optimal `NeuralDual` for the problem. As here our training and validation datasets do not differ, we pass (`dataloader_source`, `dataloader_target`) for both training and validation steps. For more details on how to configer the `NeuralDualSolver`, we refer you to the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "  0%|▎                                                                                                   | 27/10000 [00:07<33:38,  4.94it/s]"
     ]
    }
   ],
   "source": [
    "neural_dual_solver = NeuralDualSolver(\n",
    "    icnn_f, icnn_g, input_dim=input_dim, num_train_iters=10000)\n",
    "neural_dual = neural_dual_solver(\n",
    "    dataloader_source, dataloader_target, dataloader_source, dataloader_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Neural Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training has completed successfully, we can evaluate the `NeuralDual` on unseen incoming data. We first sample a new batch from the source and target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = next(dataloader_source).numpy()\n",
    "data_target = next(dataloader_target).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the corresponding transport from source to target using the gradient of the learning potential `NeuralDual.g`, i.e., $\\nabla g(\\text{source})$, or from target to source via the gradient of the learning potential `NeuralDual.f`, i.e., $\\nabla f(\\text{target})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ot_map(neural_dual, data_source, data_target, potential='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ot_map(neural_dual, data_target, data_source, potential='f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further test, how close the predicted samples are to the sampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for potential $g$, transporting source to target samples. Ideally the resulting Sinkhorn distance is close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target = neural_dual.transport(data_source, 'g')\n",
    "print(f'Sinkhorn distance between predictions and data samples: {sinkhorn_loss(pred_target, data_target)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for potential $f$, transporting target to source samples. Again, the resulting Sinkhorn distance needs to be close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_source = neural_dual.transport(data_target, 'f')\n",
    "print(f'Sinkhorn distance between predictions and data samples: {sinkhorn_loss(pred_source, data_source)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides computing the transport and mapping source to target samples or vice versa, we can also compute the overall distance between new source and target samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_dist = neural_dual.distance(data_source, data_target)\n",
    "print(f'Neural dual distance between source and target data: {dual_dist}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which compares to the primal Sinkhorn distance in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinkhorn_dist = sinkhorn_loss(data_source, data_target)\n",
    "print(f'Sinkhorn distance between source and target data: {sinkhorn_dist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b37caf44d0318b4f4d9ee96c84a0e4fe372b1526393be3417b3365184e480b09"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3910jvsc74a57bd0ea807078d9ccfe68c5cfdd69050d0d60a8848a206db173c5545cb0ff948779de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
